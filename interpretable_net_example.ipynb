{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from interpretable_net import BinaryClassificationNet\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and split into labelled and unlabelled training data and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X_train = X[:380]\n",
    "y_train = y[:380]\n",
    "X_val = X[380:]\n",
    "y_val = y[380:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define network parameters\n",
    "layers = [{'dim': 20, 'activation': tf.nn.relu, 'dropout_rate': 0.5, 'batch_norm': True},\n",
    "         {'dim': 20, 'activation': tf.nn.relu, 'dropout_rate': 0.5, 'batch_norm': True}]\n",
    "initialiser = tf.keras.initializers.glorot_uniform()\n",
    "l2_reg = 0.01\n",
    "optimiser = tf.train.AdamOptimizer()\n",
    "lambda_u = 0.01\n",
    "\n",
    "# initialise network\n",
    "model = BinaryClassificationNet(layers=layers,\n",
    "                                 initialiser=initialiser,\n",
    "                                 l2_reg=l2_reg,\n",
    "                                 optimiser=optimiser,\n",
    "                                 lambda_u=lambda_u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train  for an initial 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------+\n",
      "Running epoch 1 of 10\n",
      "Training loss = 0.6034808144659588\n",
      "Validation loss = 0.6291120493065112\n",
      "+-----------------------------------------------------------+\n",
      "Running epoch 2 of 10\n",
      "Training loss = 0.5415261851231519\n",
      "Validation loss = 0.5624726327165724\n",
      "+-----------------------------------------------------------+\n",
      "Running epoch 3 of 10\n",
      "Training loss = 0.4988272597130976\n",
      "Validation loss = 0.5187542240613352\n",
      "+-----------------------------------------------------------+\n",
      "Running epoch 4 of 10\n",
      "Training loss = 0.45677981256556355\n",
      "Validation loss = 0.47748963645210973\n",
      "+-----------------------------------------------------------+\n",
      "Running epoch 5 of 10\n",
      "Training loss = 0.4165160152914101\n",
      "Validation loss = 0.4374740448578325\n",
      "+-----------------------------------------------------------+\n",
      "Running epoch 6 of 10\n",
      "Training loss = 0.3832366534067612\n",
      "Validation loss = 0.40580041196059297\n",
      "+-----------------------------------------------------------+\n",
      "Running epoch 7 of 10\n",
      "Training loss = 0.3555652949321819\n",
      "Validation loss = 0.3788486986406266\n",
      "+-----------------------------------------------------------+\n",
      "Running epoch 8 of 10\n",
      "Training loss = 0.33040447382648525\n",
      "Validation loss = 0.35563236324205283\n",
      "+-----------------------------------------------------------+\n",
      "Running epoch 9 of 10\n",
      "Training loss = 0.31055160973858287\n",
      "Validation loss = 0.3366851917167426\n",
      "+-----------------------------------------------------------+\n",
      "Running epoch 10 of 10\n",
      "Training loss = 0.2923017866370317\n",
      "Validation loss = 0.320042338843147\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, \n",
    "        y_train, \n",
    "        X_val,\n",
    "        y_val,\n",
    "        batch_size=32, \n",
    "        num_epochs=10, \n",
    "        patience=10,\n",
    "        verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train for a further 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/my_model_final.ckpt\n",
      "+-----------------------------------------------------------+\n",
      "Running epoch 1 of 10\n",
      "Training loss = 0.27415862040055033\n",
      "Validation loss = 0.30106231310803977\n",
      "+-----------------------------------------------------------+\n",
      "Running epoch 2 of 10\n",
      "Training loss = 0.2574526110412798\n",
      "Validation loss = 0.2842581714263245\n",
      "+-----------------------------------------------------------+\n",
      "Running epoch 3 of 10\n",
      "Training loss = 0.24424794217828955\n",
      "Validation loss = 0.2697960946206299\n",
      "+-----------------------------------------------------------+\n",
      "Running epoch 4 of 10\n",
      "Training loss = 0.2345743621595351\n",
      "Validation loss = 0.2593972457224188\n",
      "+-----------------------------------------------------------+\n",
      "Running epoch 5 of 10\n",
      "Training loss = 0.22534233734814302\n",
      "Validation loss = 0.2503079193291284\n",
      "+-----------------------------------------------------------+\n",
      "Running epoch 6 of 10\n",
      "Training loss = 0.21569826481588136\n",
      "Validation loss = 0.23924116115270566\n",
      "+-----------------------------------------------------------+\n",
      "Running epoch 7 of 10\n",
      "Training loss = 0.20584410316874518\n",
      "Validation loss = 0.22831694404554564\n",
      "+-----------------------------------------------------------+\n",
      "Running epoch 8 of 10\n",
      "Training loss = 0.19630693046124897\n",
      "Validation loss = 0.21837322831291844\n",
      "+-----------------------------------------------------------+\n",
      "Running epoch 9 of 10\n",
      "Training loss = 0.1893891553391433\n",
      "Validation loss = 0.21097471381723326\n",
      "+-----------------------------------------------------------+\n",
      "Running epoch 10 of 10\n",
      "Training loss = 0.18240660078131202\n",
      "Validation loss = 0.2018828243988864\n"
     ]
    }
   ],
   "source": [
    "# continue training\n",
    "model.fit(X_train, \n",
    "        y_train, \n",
    "        X_val,\n",
    "        y_val,\n",
    "        batch_size=32, \n",
    "        num_epochs=10, \n",
    "        patience=10,\n",
    "        verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute training and validation loss (should be same as last epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/my_model_final.ckpt\n",
      "INFO:tensorflow:Restoring parameters from /tmp/my_model_final.ckpt\n",
      "Training loss = 0.1824066000441444\n",
      "Validation loss = 0.20188282284603076\n"
     ]
    }
   ],
   "source": [
    "p_train = model.predict(X_train)\n",
    "loss_train = log_loss(y_train, p_train, eps=1e-5)\n",
    "\n",
    "p_val = model.predict(X_val)\n",
    "loss_val = log_loss(y_val, p_val, eps=1e-5)\n",
    "\n",
    "print('Training loss =', loss_train)\n",
    "print('Validation loss =', loss_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Get partial derivatives of output probability wrt inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/my_model_final.ckpt\n",
      "dp / d( mean radius ) = -0.02006579195149243\n",
      "dp / d( mean texture ) = -0.0061539530346635726\n",
      "dp / d( mean perimeter ) = -0.008221661516775688\n",
      "dp / d( mean area ) = -0.023271288792602717\n",
      "dp / d( mean smoothness ) = -0.013939174559588234\n",
      "dp / d( mean compactness ) = -0.028622552861149113\n",
      "dp / d( mean concavity ) = -0.010223967162892223\n",
      "dp / d( mean concave points ) = -0.006153666534616301\n",
      "dp / d( mean symmetry ) = -0.006066223220356429\n",
      "dp / d( mean fractal dimension ) = -0.00920400924126928\n",
      "dp / d( radius error ) = -0.004189088875621868\n",
      "dp / d( texture error ) = -0.00784769964326794\n",
      "dp / d( perimeter error ) = -0.005523745273239911\n",
      "dp / d( area error ) = -0.0006209536872726555\n",
      "dp / d( smoothness error ) = -0.01739777942420915\n",
      "dp / d( compactness error ) = -0.011702424671966583\n",
      "dp / d( concavity error ) = -0.027382021381830175\n",
      "dp / d( concave points error ) = -0.011587683057102064\n",
      "dp / d( symmetry error ) = -0.004769313265569508\n",
      "dp / d( fractal dimension error ) = -0.013002311872939269\n",
      "dp / d( worst radius ) = -0.0012660615728236734\n",
      "dp / d( worst texture ) = -0.013230754671773563\n",
      "dp / d( worst perimeter ) = -0.013777974667027593\n",
      "dp / d( worst area ) = -0.010135776126602044\n",
      "dp / d( worst smoothness ) = -0.004191500373417511\n",
      "dp / d( worst compactness ) = -0.014883603846343855\n",
      "dp / d( worst concavity ) = -0.023282094703366358\n",
      "dp / d( worst concave points ) = -0.010095590527635067\n",
      "dp / d( worst symmetry ) = -0.00800237424361209\n",
      "dp / d( worst fractal dimension ) = -0.014507808934043472\n"
     ]
    }
   ],
   "source": [
    "dp_dX_val = model.get_dp_dX(X_val)\n",
    "dp_dX_val_mean = dp_dX_val.mean(axis=1)\n",
    "feature_names = load_breast_cancer()['feature_names']\n",
    "for i, feature in enumerate(feature_names):\n",
    "    print('dp /', 'd(', feature, ') =', dp_dX_val_mean[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
